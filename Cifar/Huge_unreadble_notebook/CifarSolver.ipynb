            # mask = torch.zeros_like(input)
            # mask[:, 0, :4, 0] = 1{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pylab \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from easydict import EasyDict as edict\n",
    "import pickle\n",
    "import bz2\n",
    "from collections import Counter \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DETAILS_ACC = 'details/'\n",
    "DETAILS_MASK = 'details_mask/'\n",
    "DETAILS_BIN_MASK = 'details_bin_mask/'\n",
    "AVG_RESULTS = 'averaged_results/'\n",
    "SAVED_MODELS = 'saved_models/'\n",
    "LAST_MODEL = 'last_models/'\n",
    "DATA_PATH = 'data/'\n",
    "SRC_PATH = ''\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Utils, utils and utils..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_accuracies(to_draw, args, test_name='B_lr=0.01'):\n",
    "    x = np.arange(len(to_draw['train_acc_m']))\n",
    "    \n",
    "    y1 = to_draw['train_acc']\n",
    "    y2 = to_draw['test_hint_acc']\n",
    "    y3 = to_draw['test_pure_acc']\n",
    "    \n",
    "    z1 = to_draw['train_acc_m']\n",
    "    z2 = to_draw['test_hint_acc_m']\n",
    "    z3 = to_draw['test_pure_acc_m']\n",
    "    \n",
    "    u1 = to_draw['train_acc_m_bin']\n",
    "    u2 = to_draw['test_hint_acc_m_bin']\n",
    "    u3 = to_draw['test_pure_acc_m_bin']\n",
    "    \n",
    "    plt.title(test_name, color='red')\n",
    "    plt.plot(x, y1, 'b--', label='train acc')\n",
    "    plt.plot(x, y2, 'r--', label='test hint acc')\n",
    "    plt.plot(x, y3, 'g--', label='test pure acc')\n",
    "    \n",
    "    plt.plot(x, z1, 'b:', label='train acc mask')\n",
    "    plt.plot(x, z2, 'r:', label='test hint acc mask')\n",
    "    plt.plot(x, z3, 'g:', label='test pure acc mask')\n",
    "    \n",
    "    plt.plot(x, u1, '-b', label='train acc bin_mask')\n",
    "    plt.plot(x, u2, '-r', label='test hint acc bin_mask')\n",
    "    plt.plot(x, u3, '-g', label='test pure acc bin_mask')\n",
    "    \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def get_mask(input, model, get_output=False):\n",
    "    with torch.no_grad():\n",
    "        input = input.to(device)\n",
    "        output, layers = model['classifier'](input)\n",
    "        if get_output:\n",
    "            return model['decoder'](layers), output\n",
    "\n",
    "        return model['decoder'](layers)\n",
    "\n",
    "def binarize_mask(mask, to_print):\n",
    "    with torch.no_grad():\n",
    "        avg = F.avg_pool2d(mask, 32, stride=1).squeeze()\n",
    "        flat_mask = mask.cpu().view(mask.size(0), -1)\n",
    "        binarized_mask = torch.zeros_like(flat_mask)\n",
    "        for i in range(mask.size(0)):\n",
    "            kth = 1 + int((flat_mask[i].size(0) - 1) * (1 - avg[i].item()) + 0.5)\n",
    "            th, _ = torch.kthvalue(flat_mask[i], kth)\n",
    "            th.clamp_(1e-6, 1 - 1e-6)\n",
    "            binarized_mask[i] = flat_mask[i].gt(th).float()\n",
    "        binarized_mask = binarized_mask.view(mask.size())\n",
    "        \n",
    "        if to_print:\n",
    "            print(binarized_mask.sum())\n",
    "            print(mask.squeeze()[:,:4,0])\n",
    "            print(binarized_mask.squeeze()[:,:4,0])\n",
    "        \n",
    "        return binarized_mask\n",
    "\n",
    "\n",
    "def get_binarized_mask(input, model):\n",
    "    mask = get_mask(input, model)\n",
    "    return binarize_mask(mask.clone(), False)\n",
    "\n",
    "def get_masked_images(input, binary_mask, gray_scale = 0):\n",
    "    binary_mask = binary_mask.to(device)\n",
    "    input = input.to(device)\n",
    "    with torch.no_grad():\n",
    "        if gray_scale > 0:\n",
    "            gray_background = torch.zeros_like(input) + 0.35\n",
    "            gray_background = gray_background.to(device)\n",
    "            masked_in = binary_mask * input + (1 -  binary_mask) * gray_background\n",
    "            masked_out = (1 - binary_mask) * input + binary_mask * gray_background\n",
    "        else:\n",
    "            masked_in = binary_mask * input\n",
    "            masked_out = (1 - binary_mask) * input\n",
    "\n",
    "        return masked_in, masked_out\n",
    "\n",
    "def inpaint(mask, masked_image):\n",
    "    l = []\n",
    "    for i in range(mask.size(0)):\n",
    "        permuted_image = permute_image(masked_image[i], mul255=True)\n",
    "        m = mask[i].squeeze().byte().numpy()\n",
    "        inpainted_numpy = cv2.inpaint(permuted_image, m, 3, cv2.INPAINT_TELEA) #cv2.INPAINT_NS\n",
    "        l.append(transforms.ToTensor()(inpainted_numpy).unsqueeze(0))\n",
    "    inpainted_tensor = torch.cat(l, 0)\n",
    "\n",
    "    return inpainted_tensor       \n",
    "\n",
    "def permute_image(image_tensor, mul255 = False):\n",
    "    with torch.no_grad():\n",
    "        image = image_tensor.clone().squeeze().permute(1, 2, 0)\n",
    "        if mul255:\n",
    "            image *= 255\n",
    "            image = image.byte()\n",
    "\n",
    "        return image.cpu().numpy()\n",
    "    \n",
    "def scale(out):\n",
    "    return (out * 255).astype(np.uint8)\n",
    "\n",
    "def plot(data_loader, mask, args):\n",
    "    for i, (input, target) in enumerate(data_loader):\n",
    "        batch = input\n",
    "        break\n",
    "    \n",
    "    samples_to_plot = 7\n",
    "    samples = batch[:samples_to_plot]\n",
    "    masks = mask[:samples_to_plot]\n",
    "    print(f\"Mean of masks in eval is {mask.mean()}\")        \n",
    "\n",
    "    binary_mask = binarize_mask(masks.clone(), True)\n",
    "    masked_in, masked_out = get_masked_images(samples, binary_mask, 0.35)\n",
    "    inpainted = inpaint(binary_mask, masked_out)\n",
    "\n",
    "    fig, axes = plt.subplots(4, args.columns)\n",
    "    if args.columns == 4:\n",
    "        fig.subplots_adjust(bottom=-0.02, top=1.02, wspace=0.05, hspace=0.05)\n",
    "    if args.columns == 5:\n",
    "        fig.subplots_adjust(top=0.92, wspace=0.05, hspace=0.05)\n",
    "    if args.columns == 6:\n",
    "        fig.subplots_adjust(top=0.8, wspace=0.05, hspace=0.05)\n",
    "    if args.columns == 7:\n",
    "        fig.subplots_adjust(top=0.7, wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for col in range(args.columns):\n",
    "        print(f\"Ex {col} have mean mask {masks[col].mean()}\")\n",
    "        axes[0, col].imshow(scale(permute_image(samples[col])))\n",
    "        axes[1, col].imshow(scale(permute_image(masked_in[col])))\n",
    "        axes[2, col].imshow(scale(permute_image(masked_out[col])))\n",
    "        axes[3, col].imshow(scale(permute_image(inpainted[col])))\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.gcf()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Train utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def adjust_learning_rate(optimizer, model_name, barriers, epoch, args): #150, 250 lub 165, 275\n",
    "    for param_group in optimizer[model_name].param_groups:\n",
    "        if epoch < barriers[0]:\n",
    "            param_group['lr'] = 0.01\n",
    "        elif epoch >= barriers[0] and epoch < barriers[1]:\n",
    "            param_group['lr'] = 0.001\n",
    "        else:\n",
    "            param_group = 0.0001        \n",
    "\n",
    "def save_checkpoint(state, args, path):\n",
    "    filename = (path + '.chk')\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(optimizer, classifier, decoder, filename):\n",
    "    checkpoint_dict = torch.load(filename)\n",
    "    epoch = checkpoint_dict['epoch']\n",
    "    classifier.load_state_dict(checkpoint_dict['state_dict_classifier'])\n",
    "    decoder.load_state_dict(checkpoint_dict['state_dict_decoder'])\n",
    "    if optimizer['classifier'] is not None:\n",
    "        optimizer['classifier'].load_state_dict(checkpoint_dict['optimizer_classifier'])\n",
    "    if optimizer['decoder'] is not None:\n",
    "        optimizer['decoder'].load_state_dict(checkpoint_dict['optimizer_decoder'])\n",
    "    return epoch\n",
    "\n",
    "def set_args(args):\n",
    "    args.saving_path = os.path.join(args.saving_path, args.name)\n",
    "    args.averaged_results = os.path.join(args.averaged_results, args.name) + '.log'\n",
    "    args.details_acc = os.path.join(args.details_acc, args.name) + '.log'\n",
    "    args.details_mask = os.path.join(args.details_mask, args.name) + '.log'\n",
    "    args.details_bin_mask = os.path.join(args.details_bin_mask, args.name) + '.log'\n",
    "    args.last_model = os.path.join(args.last_model, args.name)\n",
    "\n",
    "    if args.reproduce != '':\n",
    "        set_reproduction(args)\n",
    "        \n",
    "def logging_averaged(args, tr_s, val_hint, val_pure):\n",
    "    with open(args.averaged_results, 'a') as f:\n",
    "        f.write(tr_s['acc'] + ' ' + val_hint['acc'] + ' ' + val_pure['acc'] + ' ' +\n",
    "                tr_s['acc_m'] + ' ' + val_hint['acc_m'] + ' ' + val_pure['acc_m'] + ' ' +\n",
    "                tr_s['acc_m_bin'] + ' ' + val_hint['acc_m_bin'] + ' ' + val_pure['acc_m_bin'])\n",
    "\n",
    "def update_file(update_path, line):\n",
    "    with open(update_path, 'a') as f:\n",
    "        f.write(line)\n",
    "        \n",
    "def append_epoch_to_backup_files(args, epoch):\n",
    "    line_to_append = f'\\nEpoch {epoch}\\n'\n",
    "\n",
    "    update_file(args.details_acc, line_to_append)\n",
    "    update_file(args.details_mask, line_to_append)\n",
    "    update_file(args.details_bin_mask, line_to_append)    \n",
    "      \n",
    "def get_path_to_update(args, version):\n",
    "    path = args.details_acc\n",
    "    if version == 'test_hint': path = args.details_mask\n",
    "    if version == 'test_pure': path = args.details_bin_mask\n",
    "    return path\n",
    "        \n",
    "def convert_runtime_backups_into_strings(lines):\n",
    "    return '\\n'.join(lines)\n",
    "    \n",
    "def update_backup_after_epoch(args, version, backup_lines):\n",
    "    epoch_as_string = convert_runtime_backups_into_strings(backup_lines)\n",
    "    updating_path = get_path_to_update(args, version)\n",
    "    # append \\n to epoch_as_string?\n",
    "    update_file(updating_path, epoch_as_string)\n",
    "    \n",
    "def create_line(args, version, batch_accuracy, masked_accuracy, bin_masked_accuracy):\n",
    "    line = f'{batch_accuracy} {masked_accuracy} {bin_masked_accuracy}'\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arguments_setup():\n",
    "    randomhash = ''.join(str(time.time()).split('.'))\n",
    "    args = edict({\n",
    "        'data': CIFAR_PATH,\n",
    "        'n_classes': 10,\n",
    "        'no_cuda': False,\n",
    "        'seed': 11,\n",
    "        'save_model': False,\n",
    "        'saving_path': SAVED_MODELS,\n",
    "        'details_acc': DETAILS_ACC,\n",
    "        'details_mask': DETAILS_MASK,\n",
    "        'details_bin_mask': DETAILS_BIN_MASK,\n",
    "        'averaged_results': AVG_RESULTS,\n",
    "        'last_model': LAST_MODEL,\n",
    "        'name': randomhash+'random',\n",
    "        'print_freq': 100,\n",
    "        'freq_plot': 7,\n",
    "        'workers': 0,\n",
    "        'epochs': 60,\n",
    "        'batch_size': 128,\n",
    "        'pot': 0.2,\n",
    "        'lr': 0.001,\n",
    "        'lr_casme': 0.001,\n",
    "        'lrde': 115, \n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 5e-4,\n",
    "        'upsample': 'nearest',\n",
    "        'fixed_classifier': False,\n",
    "        'hp': 0.5,\n",
    "        'smf': 1000,\n",
    "        'f_size': 30,\n",
    "        'lambda_r': 10,\n",
    "        'adversarial': False,\n",
    "        'reproduce': '',\n",
    "        'hint_prob': 0.,\n",
    "        'columns': 7,\n",
    "        'plots': 16,\n",
    "        'plots_after': 5\n",
    "    })\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HintImageDatasetWrapper(Dataset):\n",
    "\n",
    "    def __init__(self, dataset: Dataset, n_classes: int, hint_prob: float = 0.):\n",
    "        self.dataset = dataset\n",
    "        self.n_classes = n_classes\n",
    "        self.hint_prob = float(hint_prob)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        img, target = self.dataset.__getitem__(index)\n",
    "        return self._annotate(img, target), target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset.__len__()\n",
    "\n",
    "    def _annotate(self, img, c):\n",
    "        code_len = int(np.ceil(np.log2(self.n_classes)))\n",
    "        assert code_len < img.size(1)\n",
    "        \n",
    "        if torch.rand(1) < self.hint_prob:\n",
    "            img[0, :code_len, 0] = 1.\n",
    "            # 255 czy 1, zmiana 1 do 10\n",
    "            i = 0\n",
    "            mod_channel = 0\n",
    "            while c > 0:\n",
    "                if c % 2 == 1:\n",
    "                    img[0, i, 0] = 2.\n",
    "                c //= 2\n",
    "                i += 1\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Imagenet32 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pickle(data, path):\n",
    "    #sfile = bz2.BZ2File('smallerfile', 'w')\n",
    "    pickle.dump({\"data\": data}, open(path, 'wb'))\n",
    "\n",
    "def unpickle_valid(data_folder):\n",
    "    data_file = os.path.join(data_folder, 'val_data')\n",
    "    with open(data_file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict['data'], dict['labels']\n",
    "    \n",
    "def unpickle_train(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict['data'], dict['labels'], dict['mean']\n",
    "\n",
    "def load_databatch(data_file, img_size=32):\n",
    "    x, y, mean_image = unpickle_train(data_file)\n",
    "\n",
    "    x = x/np.float32(255)\n",
    "    mean_image = mean_image/np.float32(255)\n",
    "    x -= mean_image\n",
    "    \n",
    "    # Labels are indexed from 1, shift it so that indexes start at 0\n",
    "    y = [i-1 for i in y]\n",
    "    data_size = x.shape[0]\n",
    "\n",
    "    img_size2 = img_size * img_size\n",
    "\n",
    "    x = np.dstack((x[:, :img_size2], x[:, img_size2:2*img_size2], x[:, 2*img_size2:]))\n",
    "    x = x.reshape((x.shape[0], img_size, img_size, 3)).transpose(0, 3, 1, 2)\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:data_size, :, :, :]\n",
    "    Y_train = y[0:data_size]\n",
    "    X_train_flip = X_train[:, :, :, ::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train, X_train_flip), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, Y_train_flip), axis=0)\n",
    "    \n",
    "    return X_train.astype(float), Y_train.astype('int32'), mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet32 Loader            \n",
    "\n",
    "\n",
    "\n",
    "# X, Y = torch.stack(X_train), torch.stack(Y_train)\n",
    "# print(f\"Full train_shapes : {X.shape}, {Y.shape}\")\n",
    "# valid, valid_labels = unpickle_valid(data_folder)\n",
    "# print(f\"Validation set shapes {valid.shape} and {valid_labels.shape}\" )\n",
    "# valid, valid_labels = torch.from_numpy(valid), torch.from_numpy(valid_labels)\n",
    "# np.save(labels_path, batch_labels)\n",
    "# test_arr = np.load(labels_path)\n",
    "# torch.save(tensor, 'file.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class ImageNet32PartDataset:\n",
    "    def __init__(self, folder_path, idx):\n",
    "        self.filepath = os.path.join(folder_path, f'train_data_batch_{idx}')\n",
    "        self.idx = idx\n",
    "        self.length = 256232\n",
    "        self.indices = torch.randperm(self.length)\n",
    "    \n",
    "    def next_part(self):\n",
    "        elements_per_part = self.length # // self.n_parts\n",
    "        X, Y, _ = load_databatch(self.filepath)\n",
    "        print(X.shape, Y.shape)\n",
    "        X, Y = X[:8], Y[:8]\n",
    "        print(X.shape, Y.shape)\n",
    "        X1, Y1 = X[0:self.length], Y[0:self.length]\n",
    "        return X1, Y1\n",
    "\n",
    "def load_test_data(idx):\n",
    "    test_path = os.path.join(TESTS_PATH, f\"{idx}/data.npy\")\n",
    "    test_arr = np.load(test_path)\n",
    "    return test_arr\n",
    "\n",
    "def data_loading():\n",
    "    n_class = 1000\n",
    "    train_dataset_parts = 10\n",
    "    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "    parts, size = 96, 256232 // 96\n",
    "    DATA_PATH = './data/'\n",
    "    \n",
    "    for idx in range(train_dataset_parts):\n",
    "        path = os.path.join(DATA_PATH, f\"{idx+1}\")\n",
    "        os.mkdir(path)\n",
    "        dataset = ImageNet32PartDataset(DATA_PATH, idx+1)\n",
    "        X, Y = dataset.next_part()\n",
    "        print(f\"{idx}-th dataset\")\n",
    "        np.random.permutation(Y.shape[0])\n",
    "        cnt = Counter(Y)\n",
    "        print(cnt)\n",
    "        for i in range(parts):\n",
    "            print(f\"{idx}/{i}\")\n",
    "\n",
    "            batch = X[i*size: (i+1)*size]\n",
    "            batch_labels = Y[i*size: (i+1)*size]\n",
    "            labels_path = os.path.join(path, f\"labels_{i}.pt\")\n",
    "            save_as_pickle(batch_labels, labels_path)\n",
    "            \n",
    "            features_path = os.path.join(path, f\"train_{i}.pt\") #.npy\n",
    "            save_as_pickle(batch, features_path)\n",
    "            batch, batch_labels = None, None\n",
    "\n",
    "        X, Y, dataset = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet32 Wrapper\n",
    "\n",
    "\n",
    "\n",
    "# def get_loader():\n",
    "#     return torch.utils.data.DataLoader(\n",
    "#         RandomFileGenerator(960, 1),\n",
    "#         batch_size=1, shuffle=True, num_workers=1, pin_memory=False, sampler=None\n",
    "#     )\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     rand_indices = random.sample(range(0, 960), 16)\n",
    "#     init_files = get_initial_tensor(rand_indices)\n",
    "#     print(init_files.shape)\n",
    "#     gener = get_loader()\n",
    "#     for idd, new_tensor in enumerate(gener):\n",
    "#         print(idd)\n",
    "#         id_to_remove = torch.randint(0, 16, (1, ))\n",
    "#         start = int(id_to_remove[0]) * 8\n",
    "#         end = start + 8\n",
    "#         init_files[start:end] = new_tensor\n",
    "#         # print(init_files.shape)\n",
    "#         # new_batch = [init_files[i][:8] for i in range(init_files.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFileGenerator(Dataset):\n",
    "    def __init__(self, n_files, n_rand_new, n_batches):\n",
    "        self.n_files = n_files\n",
    "        self.n_rand_new = n_rand_new\n",
    "        self.n_batches = n_batches\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        new_file_id = torch.randint(0, self.n_files, (self.n_rand_new,))\n",
    "        file_id, dir_id = new_file_id[0] // 10, new_file_id[0] % 10\n",
    "        dir_id += 1\n",
    "        file_path = os.path.join(DATA_PATH, f'{int(dir_id)}/train_{int(file_id)}.pt')\n",
    "        with open(file_path, 'rb') as fo:\n",
    "            tensor = pickle.load(fo)\n",
    "            tensor['data'] = tensor['data'][:8] # dlaczego pierwsze 8, a nie losowe 8? Pewnei do poprawy\n",
    "        return tensor['data']\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "      \n",
    "class EpochGenerator(Dataset):\n",
    "    def __init__(self, n_files, n_rand_new, n_batches, random_file_gen):\n",
    "        self.n_files = n_files\n",
    "        self.n_rand_new = n_rand_new\n",
    "        self.n_batches = n_batches\n",
    "        self.pieces = 16\n",
    "        self.random_file_gen = random_file_gen\n",
    "        self.initial_batch = self.get_initial_tensor()\n",
    "        \n",
    "    def get_initial_tensor(self):\n",
    "        rand_indices = random.sample(range(0, self.n_files), self.pieces)\n",
    "        batch = []\n",
    "        for i, idx in enumerate(rand_indices):\n",
    "            file_id, dir_id = idx // 10, idx % 10\n",
    "            dir_id += 1\n",
    "            file_path = os.path.join(DATA_PATH, f'{int(dir_id)}/train_{int(file_id)}.pt')\n",
    "            with open(file_path, 'rb') as fo:\n",
    "                rand_tensor = pickle.load(fo)\n",
    "                rand_tensor[\"data\"] = rand_tensor[\"data\"][:8]\n",
    "                batch.append(torch.from_numpy(rand_tensor[\"data\"]))\n",
    "        return torch.stack(batch).view(128, 3, 32, 32)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index == 0: return self.initial_batch\n",
    "        else:\n",
    "            new_piece_for_batch = next(self.random_file_gen) # czy to zadziala?\n",
    "            id_to_remove = torch.randint(0, 16, (self.n_rand_new, ))\n",
    "            start = int(id_to_remove[0]) * 8\n",
    "            end = start + 8\n",
    "            self.initial_batch[start:end] = new_tensor\n",
    "            return self.initial_batch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "def train_dataloader_per_epoch(args):\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "    valdir = os.path.join(args.data, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "    n_files = 960\n",
    "    n_boxes_to_rand_per_batch = 1\n",
    "        \n",
    "    #batch_size = 1\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            RandomFileGenerator(n_files, n_boxes_to_rand_per_batch),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=args.hint_prob\n",
    "        ), \n",
    "        batch_size=args.batch_size, shuffle=True, num_workers=args.workers, \n",
    "        pin_memory=False, sampler=None\n",
    "    )\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_dataloaders(args):\n",
    "    val_hint_loader = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(valdir, train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize\n",
    "                             ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=args.hint_prob),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n",
    "    \n",
    "    val_hint_loader_without_norm = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(valdir, train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                             ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=args.hint_prob),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n",
    "\n",
    "    val_pure_loader = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(valdir, train=False, download=True, # download=False\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                normalize\n",
    "                            ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=0.),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n",
    "    \n",
    "    val_pure_loader_without_norm = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(valdir, train=False, download=True, # download=False\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                            ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=0.),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n",
    "    \n",
    "    return val_hint_loader, val_hint_loader_without_norm, val_pure_loader, val_pure_loader_without_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(args):\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "    valdir = os.path.join(args.data, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(traindir, train=True, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.RandomCrop(32, padding=4),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                normalize\n",
    "                            ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=args.hint_prob),\n",
    "        batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=False, sampler=None)\n",
    "    \n",
    "    val_hint_loader = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(valdir, train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize\n",
    "                             ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=args.hint_prob),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n",
    "    \n",
    "    val_hint_loader_without_norm = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(valdir, train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                             ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=args.hint_prob),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n",
    "\n",
    "    val_pure_loader = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(valdir, train=False, download=True, # download=False\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                normalize\n",
    "                            ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=0.),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n",
    "    \n",
    "    val_pure_loader_without_norm = torch.utils.data.DataLoader(\n",
    "        HintImageDatasetWrapper(\n",
    "            datasets.CIFAR10(valdir, train=False, download=True, # download=False\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                            ])),\n",
    "            n_classes=args.n_classes,\n",
    "            hint_prob=0.),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n",
    "        \n",
    "    return train_loader, val_hint_loader, val_hint_loader_without_norm, val_pure_loader, val_pure_loader_without_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Resnet & Decoder architecture for Cifar10/Imagenet32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        expansion = 4\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * expansion, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(4, stride=0)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # layers.append(block(self.inplanes, planes, stride))\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ResNetShared(ResNet):\n",
    "    def forward(self, x):\n",
    "        l = []\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        l.append(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        l.append(x)\n",
    "        x = self.layer2(x)\n",
    "        l.append(x)\n",
    "        x = self.layer3(x)\n",
    "        l.append(x)\n",
    "        x = self.layer4(x)\n",
    "        l.append(x)\n",
    "    #if pretrained:\n",
    "    #    model.load_state_dict(model_zoo.load_url('https://downfload.pytorch.org/models/resnet18-19c8e357.pth'))\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x, l\n",
    "\n",
    "def resnet18shared(pretrained=False, **kwargs):\n",
    "    #model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) # [3, 4, 6, 3]\n",
    "    # model = ResNetShared(BasicBlock, [2, 2, 2, 2], **kwargs) # [3, 4, 6, 3]\n",
    "    model = ResNetShared(Bottleneck, [2, 2, 2, 2], **kwargs) # [3, 4, 6, 3]\n",
    "    return model\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, final_upsample_mode = 'nearest'):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.conv1x1_1 = self._make_conv1x1_upsampled(in_planes[0], 64)\n",
    "        self.conv1x1_2 = self._make_conv1x1_upsampled(in_planes[1], 64, 2)\n",
    "        self.conv1x1_3 = self._make_conv1x1_upsampled(in_planes[2], 64, 4)\n",
    "        self.conv1x1_4 = self._make_conv1x1_upsampled(in_planes[3], 64, 8)\n",
    "        self.cv = nn.Conv2d(64 + 4*64, 1, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_conv1x1_upsampled(self, inplanes, outplanes, scale_factor=None):\n",
    "        if scale_factor:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(outplanes),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(outplanes),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "    \n",
    "    def forward(self, l):\n",
    "        k = []\n",
    "        k.append(l[0])\n",
    "        x = self.conv1x1_1(l[1])\n",
    "        k.append(self.conv1x1_1(l[1]))\n",
    "        x = self.conv1x1_2(l[2])\n",
    "        k.append(self.conv1x1_2(l[2]))\n",
    "        x = self.conv1x1_3(l[3])\n",
    "        k.append(self.conv1x1_3(l[3]))\n",
    "        x = self.conv1x1_4(l[4])\n",
    "        k.append(self.conv1x1_4(l[4]))\n",
    "        cated = self.cv(torch.cat(k, 1))\n",
    "        cated = self.sigm(cated)\n",
    "        return cated\n",
    "\n",
    "def DecoderShared(**kwargs):\n",
    "    return Decoder([256, 512, 1024, 2048], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_k = {}\n",
    "\n",
    "def train_or_eval(data_loader, data_loader_w_norm, classifier, decoder, args, to_draw, version, train=False, optimizer=None, epoch=None):\n",
    "    acc, acc_m, acc_m_bin = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    backup_acc = []\n",
    "    \n",
    "    classifier_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    decoder.train() if train else decoder.eval()\n",
    "    classifier.train() if train and not args.fixed_classifier else classifier.eval()\n",
    "    \n",
    "    for i, (input, target) in enumerate(data_loader):\n",
    "        if train and i > len(data_loader) * args.pot:\n",
    "            break\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        with torch.set_grad_enabled(train and (not args.fixed_classifier)):\n",
    "            output, layers = classifier(input)\n",
    "            classifier_loss = classifier_criterion(output, target)\n",
    "\n",
    "        batch_accuracy = accuracy(output.detach(), target, topk=(1,))[0].item()    \n",
    "        acc.update(batch_accuracy, input.size(0))\n",
    "\n",
    "        if train and (not args.fixed_classifier):\n",
    "            optimizer['classifier'].zero_grad()\n",
    "            classifier_loss.backward()\n",
    "            optimizer['classifier'].step() # POTEM TO USUNAC!!!!!!!!!!!!!!!!!!!S\n",
    "            \n",
    "            ## save classifier (needed only if previous iterations are used i.e. args.hp > 0)\n",
    "            global F_k\n",
    "            if args.hp > 0 and ((i % args.smf == -1 % args.smf) or len(F_k) < 1):\n",
    "                print('Current iteration is saving, will be used in the future. ', end='', flush=True)\n",
    "                if len(F_k) < args.f_size:\n",
    "                    index = len(F_k) \n",
    "                else:\n",
    "                    index = random.randint(0, len(F_k) - 1)\n",
    "                state_dict = classifier.state_dict()\n",
    "                F_k[index] = {}\n",
    "                for p in state_dict:\n",
    "                    F_k[index][p] = state_dict[p].cpu()\n",
    "                print('There are {0} iterations stored.'.format(len(F_k)), flush=True)\n",
    "\n",
    "        '''\n",
    "        ## detach inner layers to make them be features for decoder\n",
    "        layers = [l.detach() for l in layers]\n",
    "        \n",
    "        with torch.set_grad_enabled(train):\n",
    "            ## compute mask and masked input\n",
    "            mask = decoder(layers)\n",
    "            #mask = mask.to(device)\n",
    "            input_m = input*(1-mask)\n",
    "            \n",
    "            if i == 0 and epoch % args.plots_after == 0 and train == False:\n",
    "                plot(data_loader_w_norm, mask, args)\n",
    "\n",
    "            ## randomly select classifier to be evaluated on masked image and compute output\n",
    "            if (not train) or args.fixed_classifier or (random.random() > args.hp):\n",
    "                output_m, _ = classifier(input_m)\n",
    "                update_classifier = not args.fixed_classifier\n",
    "            else:\n",
    "                try:\n",
    "                    confuser\n",
    "                except NameError:\n",
    "                    import copy\n",
    "                    confuser = copy.deepcopy(classifier)\n",
    "                index = random.randint(0, len(F_k) - 1)\n",
    "                confuser.load_state_dict(F_k[index])\n",
    "                confuser.eval()\n",
    "\n",
    "                output_m, _ = confuser(input_m)\n",
    "                update_classifier = False\n",
    "\n",
    "            classifier_loss_m = classifier_criterion(output_m, target)/2\n",
    "            masked_accuracy = accuracy(output_m.detach(), target, topk=(1,))[0].item() \n",
    "            acc_m.update(masked_accuracy, input.size(0))\n",
    "\n",
    "        bin_masked_accuracy = 0.0\n",
    "        if update_classifier:\n",
    "            binarized_mask = binarize_mask(mask.clone(), False).to(device)\n",
    "            bin_input_m = input * (1-binarized_mask)\n",
    "            bin_output_m, _ = classifier(bin_input_m)\n",
    "            classifier_loss_binarized_mask = classifier_criterion(bin_output_m, target)/2\n",
    "\n",
    "            bin_masked_accuracy = accuracy(bin_output_m.detach(), target, topk=(1,))[0].item()\n",
    "            acc_m_bin.update(bin_masked_accuracy, input.size(0))\n",
    "        \n",
    "        if train:\n",
    "            ## update classifier - compute gradient, do SGD step for masked image\n",
    "            if update_classifier:\n",
    "                #optimizer['classifier'].zero_grad()\n",
    "                classifier_loss_m.backward(retain_graph=True)\n",
    "                classifier_loss_binarized_mask.backward()\n",
    "            optimizer['classifier'].step()\n",
    "                \n",
    "            ## regularization for casme\n",
    "            _, max_indexes = output.detach().max(1)\n",
    "            _, max_indexes_m = output_m.detach().max(1)\n",
    "            correct_on_clean = target.eq(max_indexes)\n",
    "            mistaken_on_masked = target.ne(max_indexes_m)\n",
    "            nontrivially_confused = (correct_on_clean + mistaken_on_masked).eq(2).float()\n",
    "            if i < 2: print(f\"MASK MEAN {mask.mean()}\")\n",
    "            \n",
    "            mask_mean = F.avg_pool2d(mask, 32, stride=1).squeeze()\n",
    "            casme_loss = args.lambda_r * F.relu(mask_mean - 0.01).mean()\n",
    "            # casme_loss = -args.lambda_r * F.relu(nontrivially_confused - mask_mean).mean()\n",
    "            \n",
    "            ## main loss for casme\n",
    "            if args.adversarial:\n",
    "                casme_loss += -classifier_loss_m\n",
    "            else:\n",
    "                log_prob = F.log_softmax(output_m, 1)\n",
    "                prob = log_prob.exp()\n",
    "                negative_entropy = (log_prob * prob).sum(1)\n",
    "                ## apply main loss only when original images are corrected classified\n",
    "                negative_entropy_correct = negative_entropy * correct_on_clean.float()\n",
    "                casme_loss += negative_entropy_correct.mean()\n",
    "\n",
    "            ## update casme - compute gradient, do SGD step\n",
    "            optimizer['decoder'].zero_grad()\n",
    "            casme_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 10)\n",
    "            optimizer['decoder'].step()\n",
    "        '''\n",
    "        \n",
    "        #line = create_line(args, version, batch_accuracy, masked_accuracy, bin_masked_accuracy)\n",
    "        line = create_line(args, version, batch_accuracy, 0.0, 0.0)\n",
    "        backup_acc.append(line)\n",
    "        \n",
    "    if not train:\n",
    "        print(' * Prec@1 {acc.avg:.3f} Prec@1(M) {acc_m.avg:.3f} Prec@1(BM) {acc_m_bin.avg:.3f}'.format(\n",
    "            acc=acc, acc_m=acc_m, acc_m_bin=acc_m_bin))\n",
    "\n",
    "    update_backup_after_epoch(args, version, backup_acc)    \n",
    "    \n",
    "    to_draw[f\"{version}_acc\"].append(acc.avg)\n",
    "    to_draw[f\"{version}_acc_m\"].append(acc_m.avg)\n",
    "    to_draw[f\"{version}_acc_m_bin\"].append(acc_m_bin.avg)\n",
    "    \n",
    "    return {\n",
    "        'acc':str(acc.avg),\n",
    "        'acc_m':str(acc_m.avg),\n",
    "        'acc_m_bin':str(acc_m_bin.avg),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_to_draw_map():\n",
    "    return {\n",
    "        \"train_acc\" : [],\n",
    "        \"test_pure_acc\" : [],\n",
    "        \"test_hint_acc\" : [],\n",
    "        \"train_acc_m\" : [],\n",
    "        \"test_pure_acc_m\" : [],\n",
    "        \"test_hint_acc_m\" : [],\n",
    "        \"train_acc_m_bin\" : [],\n",
    "        \"test_pure_acc_m_bin\" : [],\n",
    "        \"test_hint_acc_m_bin\" : []\n",
    "    }\n",
    "\n",
    "def create_models(args):\n",
    "    print(\"=> creating models...\")\n",
    "    \n",
    "    classifier = resnet18shared(pretrained=True).to(device)\n",
    "    decoder = DecoderShared(final_upsample_mode=args.upsample).to(device)\n",
    "\n",
    "    optimizer = {}\n",
    "    optimizer['classifier'] = torch.optim.SGD(classifier.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    optimizer['decoder'] = torch.optim.Adam(decoder.parameters(), args.lr_casme, weight_decay=args.weight_decay)\n",
    "    \n",
    "    return classifier, decoder, optimizer\n",
    "\n",
    "def update_to_draw(results_by_epoch, to_draw):\n",
    "    keys = [\"train_acc\", \"test_hint_acc\", \"test_pure_acc\", \"train_acc_m\", \"test_hint_acc_m\", \"test_pure_acc_m\", \n",
    "           \"train_acc_m_bin\", \"test_hint_acc_m_bin\", \"test_pure_acc_m_bin\"]\n",
    "    for i, k in enumerate(keys):\n",
    "        to_draw[k] = results_by_epoch[i]\n",
    "    \n",
    "def read_old_averaged_results(args, to_draw):\n",
    "    with open(args.averaged_results, 'r') as backup:\n",
    "        lines = backup.readlines()\n",
    "        for line in lines:\n",
    "            results_by_epoch = line.split(\" \")\n",
    "            results_by_epoch = [float(result) for result in results_by_epoch]\n",
    "            update_to_draw(results_by_epoch, to_draw)\n",
    "\n",
    "def restore(args, classifier, decoder, optimizer, to_draw):\n",
    "    filename = 'xxx.chk'\n",
    "    last_checkpoint_file = os.path.join(LAST_MODEL, filename)\n",
    "    starting_epoch = 0\n",
    "    #starting_epoch = load_checkpoint(optimizer, classifier, decoder, last_checkpoint_file)\n",
    "    if starting_epoch != 0: \n",
    "        args.name = filename.split('.')[0]\n",
    "    set_args(args)\n",
    "    \n",
    "    if starting_epoch != 0:\n",
    "        read_old_averaged_results(args, to_draw)\n",
    "        \n",
    "    return starting_epoch\n",
    "\n",
    "def read_one_backup_file(args, path, to_draw):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Epoch\"):\n",
    "                pass\n",
    "                #insert new array\n",
    "            else:\n",
    "                results = line.split(\" \")\n",
    "                results = map(results, lambda x: float(x))\n",
    "                \n",
    "    assert(\"Dokoncz\" == \"To\")\n",
    "    \n",
    "def convert_backup_strings_into_floats(args, to_draw):\n",
    "    # prepare nine arrays to draw\n",
    "    paths_to_restore = [args.details_acc, args.details_mask, args.details_bin_mask]\n",
    "    for path in paths_to_restore:\n",
    "        read_one_backup_file(args, path, to_draw)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lambda_r=10, epochs=300, prob=0.):\n",
    "    args = arguments_setup()\n",
    "    args.lambda_r = lambda_r\n",
    "    args.epochs = epochs\n",
    "    args.hint_prob = prob\n",
    "    \n",
    "    to_draw = get_to_draw_map()\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    #torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    classifier, decoder, optimizer = create_models(args)\n",
    "    starting_epoch = restore(args, classifier, decoder, optimizer, to_draw)\n",
    "    \n",
    "    cudnn.benchmark = True\n",
    "    train_loader, val_hint_loader, val_hint_loader_w_norm, val_pure_loader, val_pure_loader_w_norm = data_loading(args)\n",
    "    best_acc, best_acc_pure, best_acc_mask = 0., 0., 0.\n",
    "    barriers = [165, 275]\n",
    "    \n",
    "    for epoch in range(starting_epoch, args.epochs):\n",
    "        print(f\"Epoch {epoch}. Results {best_acc} {best_acc_pure} {best_acc_mask}\")\n",
    "        \n",
    "        adjust_learning_rate(optimizer, 'classifier', barriers, epoch, args)\n",
    "        adjust_learning_rate(optimizer, 'decoder', barriers, epoch, args)\n",
    "        \n",
    "        vers = [\"train\", \"test_hint\", \"test_pure\"]\n",
    "        append_epoch_to_backup_files(args, epoch) # Insert start of the new epoch\n",
    "    \n",
    "        tr_s = train_or_eval(train_loader, train_loader, classifier, decoder, args, to_draw, vers[0], True, optimizer, epoch)\n",
    "        val_hint = train_or_eval(val_hint_loader, val_hint_loader_w_norm, classifier, decoder, args, to_draw, vers[1], epoch=epoch)\n",
    "        val_pure = train_or_eval(val_pure_loader, val_pure_loader_w_norm, classifier, decoder, args, to_draw, vers[2], epoch=epoch)\n",
    "        \n",
    "        if (epoch + 1) % args.freq_plot == 0:\n",
    "            plot_all_accuracies(to_draw, args)\n",
    "        \n",
    "        if float(val_pure['acc_m']) > best_acc_mask:\n",
    "            print(f\"Best result in epoch {epoch} is {float(val_pure['acc_m'])}\")\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict_classifier': classifier.state_dict(),\n",
    "                'state_dict_decoder': decoder.state_dict(),\n",
    "                'optimizer_classifier' : optimizer['classifier'].state_dict(),\n",
    "                'optimizer_decoder' : optimizer['decoder'].state_dict(),\n",
    "                'args' : args,\n",
    "            }, args, args.saving_path)\n",
    "            \n",
    "        best_acc = max(best_acc, float(val_hint['acc']))\n",
    "        best_acc_pure = max(best_acc_pure, float(val_pure['acc']))\n",
    "        best_acc_mask = max(best_acc_mask, float(val_pure['acc_m']))\n",
    "        \n",
    "        logging_averaged(args, tr_s, val_hint, val_pure)\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict_classifier': classifier.state_dict(),\n",
    "            'state_dict_decoder': decoder.state_dict(),\n",
    "            'optimizer_classifier' : optimizer['classifier'].state_dict(),\n",
    "            'optimizer_decoder' : optimizer['decoder'].state_dict(),\n",
    "            'args' : args,\n",
    "        }, args, args.last_model)\n",
    "        \n",
    "# w sensie skupilbym sie na czterech opcjach, z casme / bez casme, z p=0 i p=1\n",
    "if __name__ == '__main__':\n",
    "    main(lambda_r=12, epochs=400, prob=1.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
